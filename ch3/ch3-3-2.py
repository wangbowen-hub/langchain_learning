import os
import io
import requests
from tqdm import tqdm
from pydantic import Field
from typing import List, Mapping, Optional, Any
from langchain_core.language_models.llms import LLM
from gpt4all import GPT4All

class CustomLLM(LLM):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„LLMç±»ï¼Œç”¨äºé›†æˆgpt4allæ¨¡å‹

    å‚æ•°ï¼š

    model_folder_path: (str) å­˜æ”¾æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„
    model_name: (str) è¦ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆ<æ¨¡å‹åç§°>.binï¼‰
    allow_download: (bool) æ˜¯å¦å…è®¸ä¸‹è½½æ¨¡å‹

    backend: (str) æ¨¡å‹çš„åç«¯ï¼ˆæ”¯æŒçš„åç«¯ï¼šllama/gptjï¼‰
    n_threads: (str) è¦ä½¿ç”¨çš„çº¿ç¨‹æ•°
    n_predict: (str) è¦ç”Ÿæˆçš„æœ€å¤§tokenæ•°
    temp: (str) ç”¨äºé‡‡æ ·çš„æ¸©åº¦
    top_p: (float) ç”¨äºé‡‡æ ·çš„top-på€¼
    top_k: (float) ç”¨äºé‡‡æ ·çš„top kå€¼
    """
    # ä»¥ä¸‹æ˜¯ç±»å±æ€§çš„å®šä¹‰
    model_folder_path: str = Field(None, alias='model_folder_path')
    model_name: str = Field(None, alias='model_name')
    allow_download: bool = Field(None, alias='allow_download')

    # æ‰€æœ‰å¯é€‰å‚æ•°

    backend:        Optional[str]   = 'llama'
    temp:           Optional[float] = 0.7
    top_p:          Optional[float] = 0.1
    top_k:          Optional[int]   = 40
    n_batch:        Optional[int]   = 8
    n_threads:      Optional[int]   = 4
    n_predict:      Optional[int]   = 256

    # åˆå§‹åŒ–æ¨¡å‹å®ä¾‹
    gpt4_model_instance:Any = None 

    def __init__(self, model_folder_path, model_name, allow_download, **kwargs):
        super(CustomLLM, self).__init__()
        # ç±»æ„é€ å‡½æ•°çš„å®ç°
        self.model_folder_path: str = model_folder_path
        self.model_name = model_name
        self.allow_download = allow_download
        
        # è§¦å‘è‡ªåŠ¨ä¸‹è½½
        self.auto_download()

        # åˆ›å»ºGPT4Allæ¨¡å‹å®ä¾‹
        self.gpt4_model_instance = GPT4All(
            model_name=self.model_name,
            model_path=self.model_folder_path,
        )

    def auto_download(self) -> None:
        """
        æ­¤æ–¹æ³•å°†ä¼šä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„, å‚è€ƒhttps://python.langchain.com/docs/integrations/llms/gpt4all
        """
        # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åŒ…å«.binåç¼€æˆ–.ggufåç¼€
        if self.model_name.endswith(".bin") or self.model_name.endswith(".gguf"):
            model_name = self.model_name
        else:
            model_name = f"{self.model_name}.gguf"
            
        if not os.path.exists(self.model_folder_path):
            os.makedirs(self.model_folder_path)

        download_path = os.path.join(self.model_folder_path, model_name)

        if not os.path.exists(download_path):
            if self.allow_download:
                # å‘URLå‘é€GETè¯·æ±‚ä¸‹è½½æ–‡ä»¶
                # å› ä¸ºæ–‡ä»¶è¾ƒå¤§ï¼Œæ‰€ä»¥è¾¹ä¸‹è½½è¾¹æµå¼ä¼ è¾“
                try:
                    # ä½¿ç”¨æ–°çš„GGUFæ ¼å¼æ¨¡å‹URL
                    if self.model_name.endswith(".bin"):
                        print("è­¦å‘Šï¼šGGMLæ ¼å¼å·²å¼ƒç”¨ï¼Œæ¨èä½¿ç”¨GGUFæ ¼å¼æ¨¡å‹")
                        url = f'https://gpt4all.io/models/{model_name}'
                    else:
                        
                        url = f'https://gpt4all.io/models/{model_name}'
                        print(url)

                    response = requests.get(url, stream=True)
                    # ä»¥äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶å†™å…¥å“åº”å†…å®¹çš„å—
                    with open(download_path, 'wb') as f:
                        for chunk in tqdm(response.iter_content(chunk_size=8912)):
                            if chunk: f.write(chunk)
                
                except Exception as e:
                    print(f"=> ä¸‹è½½å¤±è´¥ã€‚é”™è¯¯: {e}")
                    return
                
                print(f"=> æ¨¡å‹: {self.model_name} å·²æˆåŠŸä¸‹è½½ ğŸ¥³")
            
            else:
                print(
                    f"æ¨¡å‹: {self.model_name} ä¸å­˜åœ¨äº {self.model_folder_path}",
                    "è¯·é€šè¿‡è®¾ç½® allow_download = True ä¸‹è½½æ¨¡å‹")
                
    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """
        è¿”å›ä¸€ä¸ªå­—å…¸ç±»å‹, åŒ…å«LLMçš„å”¯ä¸€æ ‡è¯†
        """
        return {
            'model_name' : self.model_name,
            'model_path' : self.model_folder_path,
            **self._get_model_default_parameters
        }
    
    @property
    def _llm_type(self) -> str:
        return 'gpt4all'
    
    def _call(
            self, 
            prompt: str, stop: Optional[List[str]] = None, 
            **kwargs) -> str:
        """
        é‡å†™åŸºç±»æ–¹æ³•, æ ¹æ®ç”¨æˆ·è¾“å…¥çš„promptæ¥å“åº”ç”¨æˆ·, è¿”å›å­—ç¬¦ä¸²      
        """
        
        params = {
            **self._get_model_default_parameters, 
            **kwargs
        }

        with self.gpt4_model_instance.chat_session():
            response_generator = self.gpt4_model_instance.generate(prompt, **params)

            if params['streaming']:
                response = io.StringIO()
                for token in response_generator:
                    print(token, end='', flush=True)
                    response.write(token)
                response_message = response.getvalue()
                response.close()
                return response_message
        return response_generator
    
if __name__ == "__main__":
    # æ›´æ¢ä¸ºGGUFæ ¼å¼æ¨¡å‹
    llm = CustomLLM(model_folder_path= ("./models/"), model_name="ggml-gpt4all-j-v1.3-groovy.guff", allow_download=True)
    print(llm("è®²ä¸€ä¸ªç¬‘è¯"))